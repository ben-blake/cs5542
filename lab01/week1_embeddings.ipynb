{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ben-blake/cs5542-lab01/blob/main/week1_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDBe_AyEtCeD"
      },
      "source": [
        "# CS 5542 — Week 1 Lab\n",
        "## From Data to Retrieval: GitHub → Colab → Hugging Face → Embeddings\n",
        "\n",
        "**Learning Goals:**\n",
        "- Use GitHub for collaborative analytics workflows\n",
        "- Run notebooks in Google Colab\n",
        "- Load datasets and models from Hugging Face Hub\n",
        "- Build an embedding-based retrieval system (mini-RAG)\n"
      ],
      "id": "CDBe_AyEtCeD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5vmTy_BtCeF"
      },
      "source": [
        "### GenAI Systems Context (Mini-RAG)\n",
        "This lab implements a **mini Retrieval-Augmented Generation (RAG)** pipeline:\n",
        "- A **Transformer encoder** produces semantic embeddings\n",
        "- A **vector index (FAISS)** enables fast retrieval\n",
        "- Retrieved context is what a downstream **LLM** would use for grounded generation\n"
      ],
      "id": "O5vmTy_BtCeF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEE06cSrtCeH"
      },
      "source": [
        "## Step 1 — Environment Setup\n",
        "Install required libraries. This may take ~1 minute.\n"
      ],
      "id": "bEE06cSrtCeH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93HfBIa3tCeH"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets sentence-transformers faiss-cpu"
      ],
      "id": "93HfBIa3tCeH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_20OhDwtCeJ"
      },
      "source": [
        "## Step 2 — Load Dataset & Model from Hugging Face Hub\n",
        "We use a lightweight news dataset and a sentence embedding model.\n"
      ],
      "id": "-_20OhDwtCeJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "103iS4tctCeJ"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "dataset = load_dataset(\"ag_news\", split=\"train[:200]\")\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "texts = dataset[\"text\"]\n",
        "print(f\"Loaded {len(texts)} documents\")"
      ],
      "id": "103iS4tctCeJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiIvjeqVtCeK"
      },
      "source": [
        "## Step 3 — Create Embeddings\n",
        "These vectors represent semantic meaning and enable retrieval before generation.\n"
      ],
      "id": "DiIvjeqVtCeK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qtBTR5FtCeL"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "embeddings = model.encode(texts, show_progress_bar=True)\n",
        "print('Embedding shape:', embeddings.shape)"
      ],
      "id": "4qtBTR5FtCeL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyiycncAtCeL"
      },
      "source": [
        "## Step 4 — Build a Vector Index (FAISS)\n",
        "This simulates the retrieval layer in RAG systems.\n"
      ],
      "id": "LyiycncAtCeL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3qi6YZ5tCeM"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(np.array(embeddings))\n",
        "print('Index size:', index.ntotal)"
      ],
      "id": "q3qi6YZ5tCeM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4kSCYaetCeM"
      },
      "source": [
        "## Step 5 — Retrieval Function\n",
        "Search for documents related to a query.\n"
      ],
      "id": "O4kSCYaetCeM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPMB0rGmtCeN"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def search(query, k=3):\n",
        "    q_emb = model.encode([query])\n",
        "    distances, indices = index.search(np.array(q_emb), k)\n",
        "    return [texts[int(i)] for i in indices[0]]"
      ],
      "id": "SPMB0rGmtCeN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViLMqRuMtCeO"
      },
      "source": [
        "## Step 6 — Try It!\n"
      ],
      "id": "ViLMqRuMtCeO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_OY1lkmtCeO"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "search(\"stock market and economy\")"
      ],
      "id": "K_OY1lkmtCeO"
    },
    {
      "cell_type": "code",
      "source": [
        "print(search(\"artificial intelligence in healthcare\"))"
      ],
      "metadata": {
        "id": "RZ6s4o0qvCBo"
      },
      "id": "RZ6s4o0qvCBo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w6h9jiVtCeO"
      },
      "source": [
        "## Reflection\n",
        "**In 1–2 sentences, explain how embeddings enable retrieval before generation in GenAI systems.**\n"
      ],
      "id": "6w6h9jiVtCeO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings convert text into numerical vectors where similar meanings end up close together, allowing us to quickly find the most relevant documents for any query using vector similarity search. RAG systems leverage this by fetching relevant context first and then feeding it to the LLM along with the user's question, enabling the model to generate answers grounded in actual retrieved knowledge instead of \"hallucinating\" from what it learned during training."
      ],
      "metadata": {
        "id": "DYqg-T38vbvE"
      },
      "id": "DYqg-T38vbvE"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}