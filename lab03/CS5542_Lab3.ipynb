{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3d0c34ca",
      "metadata": {
        "id": "3d0c34ca"
      },
      "source": [
        "# CS 5542 \u2014 Lab 3: Multimodal RAG Systems & Retrieval Evaluation  \n",
        "**Text + Images/PDFs (runs offline by default; optional LLM API hook)**\n",
        "\n",
        "This notebook is a **student-ready, simplified, and fully runnable** lab workflow for **multimodal retrieval-augmented generation (RAG)**:\n",
        "- ingest **PDF text** + **image captions/filenames**\n",
        "- retrieve evidence with a lightweight baseline (TF\u2011IDF)\n",
        "- build a **context block** for answering\n",
        "- evaluate retrieval quality (Precision@5, Recall@10)\n",
        "- run an **ablation study** (REQUIRED)\n",
        "\n",
        "> \u2705 **Important:** The code is optimized for **clarity + reproducibility for students** (minimal dependencies, no keys required).  \n",
        "> It is not the \u201cfastest possible\u201d or \u201cbest-performing\u201d RAG system \u2014 but it is a correct baseline that you can extend.\n",
        "\n",
        "---\n",
        "\n",
        "## Student Tasks (what you must do)\n",
        "1. **Ingest** PDFs + images from `project_data_mm/` (or use the provided sample package).  \n",
        "2. Implement / experiment with **chunking strategies** (page-based vs fixed-size).  \n",
        "3. Compare retrieval methods (at least):  \n",
        "   - **Sparse** (TF\u2011IDF / BM25-style)  \n",
        "   - **Dense** (optional: embeddings)  \n",
        "   - **Hybrid** (score fusion with `alpha`)  \n",
        "   - **Hybrid + rerank** (optional: reranker / LLM rerank)  \n",
        "4. Build a **multimodal context** that includes **evidence items** (text + images).  \n",
        "5. Produce the required **results table**:\n",
        "\n",
        "`Query \u00d7 Method \u00d7 Precision@5 \u00d7 Recall@10 \u00d7 Faithfulness`\n",
        "\n",
        "---\n",
        "\n",
        "## Expected Outputs (what graders look for)\n",
        "- Printed ingestion counts (how many PDF pages/chunks, how many images)\n",
        "- A retrieval demo showing **top\u2011k evidence** for a query\n",
        "- Evaluation metrics per method (P@5, R@10)\n",
        "- An ablation section with a small comparison table + short explanation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "734b5101",
      "metadata": {
        "id": "734b5101"
      },
      "source": [
        "## Key Parameters You Can Tune (and what they do)\n",
        "\n",
        "These parameters control retrieval + context building. **Students should change them and report what happens.**\n",
        "\n",
        "- **`TOP_K_TEXT`**: how many text chunks to consider as candidates.  \n",
        "  - Larger \u2192 more recall, but more noise (lower precision).\n",
        "- **`TOP_K_IMAGES`**: how many image items to consider as candidates.  \n",
        "  - Larger \u2192 more multimodal evidence, but can add irrelevant images.\n",
        "- **`TOP_K_EVIDENCE`**: how many total evidence items (text+image) go into the final context.  \n",
        "  - Larger \u2192 longer context; may dilute answer quality.\n",
        "- **`ALPHA`** *(0 \u2192 1)*: **fusion weight** when mixing text vs image evidence.  \n",
        "  - `ALPHA = 1.0` \u2192 text dominates  \n",
        "  - `ALPHA = 0.0` \u2192 images dominate  \n",
        "  - typical starting point: `0.5`\n",
        "- **`CHUNK_SIZE`** (fixed-size chunking): characters per chunk (baseline).  \n",
        "  - Smaller \u2192 more granular retrieval (often higher precision)  \n",
        "  - Larger \u2192 fewer chunks (often higher recall but less specific)\n",
        "- **`CHUNK_OVERLAP`**: overlap between chunks to avoid cutting important info.  \n",
        "  - Too high \u2192 redundant chunks; too low \u2192 missing context boundaries\n",
        "\n",
        "### What to try (recommended student experiments)\n",
        "- Keep everything fixed, vary **`ALPHA`**: 0.2, 0.5, 0.8  \n",
        "- Vary **`TOP_K_TEXT`**: 2, 5, 10  \n",
        "- Compare **page-based** vs **fixed-size** chunking (required ablation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fa6fe39",
      "metadata": {
        "id": "5fa6fe39"
      },
      "source": [
        "## 0) Student Info (Fill in)\n",
        "- Name: Ben Blake\n",
        "- UMKC ID: 14387365\n",
        "- Course/Section: CS5542-0001\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "311e0454",
      "metadata": {
        "id": "311e0454"
      },
      "source": [
        "## 1) Setup (student-friendly baseline)\n",
        "\n",
        "This lab starter is designed to be **easy to run** and **easy to modify**:\n",
        "- **PyMuPDF (`fitz`)** for PDF text extraction\n",
        "- **scikit-learn** for TF\u2011IDF retrieval (strong sparse baseline)\n",
        "- **Pillow** for basic image IO\n",
        "- Optional: connect an **LLM API** for answer generation (not required to run retrieval + eval)\n",
        "\n",
        "### Student guideline\n",
        "- First make sure **retrieval + metrics** run end-to-end.\n",
        "- Then iterate: chunking \u2192 retrieval method \u2192 fusion (`ALPHA`) \u2192 rerank \u2192 faithfulness.\n",
        "\n",
        "> If you have API keys (e.g., Gemini / OpenAI / etc.), you can plug them into the optional LLM hook later \u2014  \n",
        "> but your retrieval evaluation should work **without** any external keys.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "25b3d405",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25b3d405",
        "outputId": "9987152c-047e-48ea-e200-f4d96913219c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Collecting rank-bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (5.0.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.3.7)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (26.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (8.3.1)\n",
            "Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank-bm25, PyMuPDF, faiss-cpu\n",
            "Successfully installed PyMuPDF-1.26.7 faiss-cpu-1.13.2 rank-bm25-0.2.2\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os, re, glob, json, math\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "!pip install PyMuPDF sentence-transformers faiss-cpu rank-bm25\n",
        "import fitz  # PyMuPDF\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "import faiss\n",
        "from rank_bm25 import BM25Okapi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3AU4Z2H8pku"
      },
      "source": [
        "### Cell Description: Imports & Setup\n",
        "\n",
        "- **What this cell does:** Imports necessary libraries for file handling (os, glob), PDF processing (PyMuPDF), and vector operations (numpy, sklearn, faiss).\n",
        "- **Why it matters:** Sets up the environment with tools for OCR-free PDF text extraction and dense/sparse retrieval.\n",
        "- **Key assumptions:** Dependencies like `pymupdf` and `sentence-transformers` are installed in the environment."
      ],
      "id": "X3AU4Z2H8pku"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d89da50c",
      "metadata": {
        "id": "d89da50c"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Lab Configuration (EDIT ME)\n",
        "# =========================\n",
        "# Students: try changing these and observe how retrieval metrics change.\n",
        "\n",
        "DATA_DIR = \"project_data_mm\"   # folder containing pdfs/ and images/\n",
        "PDF_DIR  = os.path.join(DATA_DIR, \"pdfs\")\n",
        "IMG_DIR  = os.path.join(DATA_DIR, \"images\")\n",
        "\n",
        "# Retrieval knobs\n",
        "TOP_K_TEXT     = 5    # candidate text chunks\n",
        "TOP_K_IMAGES   = 3    # candidate images (based on captions/filenames)\n",
        "TOP_K_EVIDENCE = 8    # final evidence items used in the context\n",
        "\n",
        "# Fusion knob (text vs images)\n",
        "ALPHA = 0.5  # 0.0 = images dominate, 1.0 = text dominates\n",
        "\n",
        "# Chunking knobs (for fixed-size chunking ablation)\n",
        "CHUNK_SIZE    = 900   # characters per chunk\n",
        "CHUNK_OVERLAP = 150   # overlap characters\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_SEED = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVOTNlW28pkv"
      },
      "source": [
        "### Cell Description: Configuration\n",
        "\n",
        "- **What this cell does:** Defines global hyperparameters for the RAG pipeline, including k-retrieval counts (`TOP_K`) and fusion weights (`ALPHA`).\n",
        "- **Why it matters:** These parameters directly control the trade-off between precision (low k) and recall (high k), and the balance between text and image evidence.\n",
        "- **Tradeoffs:** Higher `TOP_K` increases recall but risks polluting the context with irrelevant noise."
      ],
      "id": "eVOTNlW28pkv"
    },
    {
      "cell_type": "markdown",
      "id": "a073bd3a",
      "metadata": {
        "id": "a073bd3a"
      },
      "source": [
        "## 2) Data folder\n",
        "Expected structure:\n",
        "```\n",
        "project_data_mm/\n",
        "  doc1.pdf\n",
        "  doc2.pdf\n",
        "  figures/\n",
        "    img1.png\n",
        "    ... (>=5)\n",
        "```\n",
        "\n",
        "If the folder is missing, we will generate **sample PDFs and images** automatically so you can run and verify the pipeline end-to-end.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eb5e694",
      "metadata": {
        "id": "2eb5e694"
      },
      "source": [
        "## 3) Define your 3 queries + rubrics\n",
        "**Guideline:** write queries that can be answered using your PDFs/images.\n",
        "\n",
        "Rubric format below is **simple and runnable**:\n",
        "- `must_have_keywords`: words/phrases that should appear in relevant evidence\n",
        "- `optional_keywords`: nice-to-have\n",
        "\n",
        "Later, retrieval metrics will treat an evidence chunk as relevant if it contains at least one `must_have_keywords` item.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "80ccdf82",
      "metadata": {
        "id": "80ccdf82"
      },
      "outputs": [],
      "source": [
        "QUERIES = [\n",
        "    {\n",
        "        \"id\": \"Q1\",\n",
        "        \"question\": \"Based on the risk matrix shown in the figures and the accompanying text, which combination of likelihood and impact corresponds to the highest risk level?\",\n",
        "        \"rubric\": {\n",
        "            \"must_have_keywords\": [\"likelihood\", \"impact\", \"high risk\"],\n",
        "            \"optional_keywords\": [\"risk matrix\", \"heat map\", \"severity\", \"probability\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"Q2\",\n",
        "        \"question\": \"Using both the Zero Trust architecture diagram and the document text, what core principle is emphasized for access decisions?\",\n",
        "        \"rubric\": {\n",
        "            \"must_have_keywords\": [\"zero trust\", \"verify\", \"never trust\"],\n",
        "            \"optional_keywords\": [\"least privilege\", \"continuous authentication\", \"identity\", \"access control\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"Q3\",\n",
        "        \"question\": \"What specific encryption algorithm (for example, AES-256 or RSA-2048) is mandated by the organization\u2019s policy?\",\n",
        "        \"rubric\": {\n",
        "            \"must_have_keywords\": [\"AES-256\", \"RSA-2048\", \"encryption algorithm\"],\n",
        "            \"optional_keywords\": [\"policy\", \"standard\", \"cryptographic\"]\n",
        "        }\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUeBGBF28pkx"
      },
      "source": [
        "### Cell Description: Evaluation Dataset\n",
        "\n",
        "- **What this cell does:** Defines the test set of queries with a ground-truth rubric (keywords) for evaluation.\n",
        "- **Why it matters:** Essential for objective evaluation of the retrieval system using Precision and Recall metrics.\n",
        "- **Key assumptions:** The queries cover both text and image modalities present in the dataset."
      ],
      "id": "EUeBGBF28pkx"
    },
    {
      "cell_type": "markdown",
      "id": "5ddd9add",
      "metadata": {
        "id": "5ddd9add"
      },
      "source": [
        "## 4) Ingestion\n",
        "We extract:\n",
        "- **PDF per-page text** as `TextChunk`\n",
        "- **Image metadata** as `ImageItem` (caption = filename without extension)\n",
        "\n",
        "> This is intentionally lightweight so it runs without downloading large embedding models.\n",
        "\n",
        "\n",
        "**Cell Description:**\n",
        "This cell handles the ingestion of PDF documents and images. We implement two chunking strategies: page-based (natural boundaries) and fixed-size (consistent length). This is crucial for RAG as the chunk size determines the context window usage and semantic completeness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "560eb7b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "560eb7b7",
        "outputId": "7f181eb0-fcf2-47ef-9b4c-3b2e5afe1a98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total page chunks: 221\n",
            "Total fixed chunks: 859\n",
            "Total images: 10\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class TextChunk:\n",
        "    chunk_id: str\n",
        "    doc_id: str\n",
        "    page_num: int\n",
        "    text: str\n",
        "\n",
        "@dataclass\n",
        "class ImageItem:\n",
        "    item_id: str\n",
        "    path: str\n",
        "    caption: str\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    s = s or \"\"\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def extract_pdf_pages(pdf_path: str) -> List[TextChunk]:\n",
        "    doc_id = os.path.basename(pdf_path)\n",
        "    doc = fitz.open(pdf_path)\n",
        "    out: List[TextChunk] = []\n",
        "    for i in range(len(doc)):\n",
        "        page = doc.load_page(i)\n",
        "        text = clean_text(page.get_text(\"text\"))\n",
        "        if text:\n",
        "            out.append(TextChunk(chunk_id=f\"{doc_id}::p{i+1}\", doc_id=doc_id, page_num=i+1, text=text))\n",
        "    return out\n",
        "\n",
        "def chunk_text_fixed(text: str, chunk_size: int, overlap: int, doc_id: str, page_num: int) -> List[TextChunk]:\n",
        "    chunks = []\n",
        "    if not text: return chunks\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(start + chunk_size, len(text))\n",
        "        chunk_text = text[start:end]\n",
        "        chunks.append(TextChunk(chunk_id=f\"{doc_id}::p{page_num}::c{start}\", doc_id=doc_id, page_num=page_num, text=chunk_text))\n",
        "        start += (chunk_size - overlap)\n",
        "        if start >= len(text): break\n",
        "    return chunks\n",
        "\n",
        "def extract_pdf_fixed(pdf_path: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[TextChunk]:\n",
        "    doc_id = os.path.basename(pdf_path)\n",
        "    doc = fitz.open(pdf_path)\n",
        "    out: List[TextChunk] = []\n",
        "    for i in range(len(doc)):\n",
        "        page = doc.load_page(i)\n",
        "        text = clean_text(page.get_text(\"text\"))\n",
        "        if text:\n",
        "            out.extend(chunk_text_fixed(text, chunk_size, overlap, doc_id, i+1))\n",
        "    return out\n",
        "\n",
        "def load_images(fig_dir: str) -> List[ImageItem]:\n",
        "    items: List[ImageItem] = []\n",
        "    for p in sorted(glob.glob(os.path.join(fig_dir, \"*.*\"))):\n",
        "        base = os.path.basename(p)\n",
        "        caption = os.path.splitext(base)[0].replace(\"_\", \" \")\n",
        "        items.append(ImageItem(item_id=base, path=p, caption=caption))\n",
        "    return items\n",
        "\n",
        "pdfs = sorted(glob.glob(os.path.join(PDF_DIR, \"*.pdf\")))\n",
        "page_chunks = []\n",
        "for p in pdfs: page_chunks.extend(extract_pdf_pages(p))\n",
        "fixed_chunks = []\n",
        "for p in pdfs: fixed_chunks.extend(extract_pdf_fixed(p))\n",
        "image_items = load_images(IMG_DIR)\n",
        "\n",
        "print(\"Total page chunks:\", len(page_chunks))\n",
        "print(\"Total fixed chunks:\", len(fixed_chunks))\n",
        "print(\"Total images:\", len(image_items))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-a2-uQh8pky"
      },
      "source": [
        "### Cell Description: Ingestion & Chunking\n",
        "\n",
        "- **What this cell does:** Extracts text from PDFs and captions from images. Implements two chunking strategies: page-based (natural boundaries) and fixed-size (consistent length).\n",
        "- **Why it matters:** Standardizes raw documents into a format suitable for indexing. The choice of chunking strategy impacts context window usage and semantic coherence.\n",
        "- **Tradeoffs:** Page-based chunking preserves document structure but varies in size; fixed-size ensures consistency but may split sentences."
      ],
      "id": "c-a2-uQh8pky"
    },
    {
      "cell_type": "markdown",
      "id": "cf833eaf",
      "metadata": {
        "id": "cf833eaf"
      },
      "source": [
        "## 5) Retrieval (TF\u2011IDF)\n",
        "We build two TF\u2011IDF indexes:\n",
        "- One over **PDF text chunks**\n",
        "- One over **image captions**\n",
        "\n",
        "Retrieval returns the top\u2011k results with similarity scores.\n",
        "\n",
        "\n",
        "**Cell Description:**\n",
        "We build three types of indices: Sparse (TF-IDF, BM25) and Dense (SentenceTransformers). This allows us to compare keyword-based retrieval against semantic retrieval. We also index image captions to enable multimodal retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f9fde54d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554,
          "referenced_widgets": [
            "6053429fcf774bb1a81334d51907fc3d",
            "ccceca2af5ad420c91c7242c032e4a05",
            "8071fe7e5dc34e03b53f319324c1cee6",
            "2aff0873f6d54549a86652dec9d0ede6",
            "cf8c7be4adec4c12ad025c059fbf888b",
            "7f05e8c6b9de434ab6dcc0a78f60b137",
            "f3e22f180800415b96c56960f7c54a39",
            "7fc640c43110417ba3db84c6bf1ffd6b",
            "4f073c94f1ca4a2aac2b223879f95eb1",
            "9441a5bc6ba44201b5b24022cae389f7",
            "5532739dd2d14aabac07b32596cd5e16",
            "f9a0f5e9fbf145fbbcbac9403b2ac49d",
            "554bfa9ce60e42de94c82541997157b8",
            "b9a1506bfe564f29b40935df2353820c",
            "de6f874bd5374344a7c669d58eaf87a6",
            "c207e4700d7e4833af3830487fb9e0b2",
            "3ce63dfaccd0441c857666d1eb330bbf",
            "0fe005ef53464e41a09fee84fba8d8f2",
            "58176875e083425fb6644d766ca89000",
            "934cc52e4ae24cba9e07c3eb34185291",
            "b3c1b3f15ab4463b899c758f73484949",
            "2990421e6ad44758bcc346f449100114",
            "60d95e191a134dc7a48a2f9c81084537",
            "42a5116f0bce48dcac95622e6c0c08c4",
            "7533f0145edd499fbbbdb1dedd477f62",
            "0bb0c2df45fe4080acfc15c922db5a1b",
            "4ef2db3c75654b2b9bf1ebb0e7671352",
            "e312241355864df2af79ac1e1daebb84",
            "52a47077cc574775ac7ddffe43a0faf3",
            "445805d094fc43bea8d23d23fb82edd2",
            "8510915908a1406bb3d40a0ec8c6a723",
            "d14acffb71084a5e86742f02bd66acff",
            "0afe9785449344569ccf50be14ec3361",
            "2e571033cdb843fc88f3027de180f3b6",
            "7388d13b05be4725960bfd2760e82a66",
            "89f7eef47226410d8c9e0ad3a2f0157c",
            "1815766b77d643fd94c736b3bdcc1948",
            "b221395142264f66ac57c75be165a2a3",
            "cf48e1b4868a431bb797825f58369345",
            "b7debd94e62a43dcbe19c789c43c5fb8",
            "588e31ba3f3242218daeee6a009210f9",
            "5694d306f3754cf5911519bb6bad80f6",
            "56ff2fc5dbcb448faba4e8f421bff4ec",
            "d1f8955fbe774ec8ba18161f9dcae8f6",
            "03dea5a1c59f40a88046a2ad630cc00d",
            "00b55b0f6b9b4dccb3651f3543e1455b",
            "3391a0dd08df411d978f8c5dee671fac",
            "0fb8c215b4bc437883350c1c0fdddd19",
            "0f0b2bcc100442568ee11d700d56e185",
            "c51d6d305769482480e035231ad16f5e",
            "6a0785ef67784ab7a4bd12df2364402d",
            "0a2f362859ef433c9fd0ef9ab15be4bb",
            "724d4089c69d434fa84a409586638750",
            "23e88b2cbffb4d0c915c2b1e2caaa337",
            "d72bdf81c70d44aca8a01192c0b2900b",
            "27eff59366f74102b6d339fd1b49c56f",
            "8a6ac991658148ed86125cb27aed9146",
            "34c318c022c24ea0ae324c576879a95a",
            "89814b5d60af45a18552aaef8eccbabf",
            "8ab4ef80113e4621b75178494dcc7728",
            "80f215c0c8e64b84aea97082a8d218d2",
            "8e500540799e40e0b188a9cb83f7544f",
            "f6ae686f889a45318faa8eada89e2b81",
            "d251abc74ffa4131aad8971e70328d37",
            "0bd48b69c33843b09b1c157296525773",
            "bcf2d04470214c62afcbfe76adbfbe62",
            "77a6f84499e145019729ff4326d94b15",
            "781fba8f1cda49bba6043c7ef3fd5373",
            "57b1d53a61364810b8291c9a0a899903",
            "189e80943ac749e2894157873cfde1ff",
            "1e82c5bee4404c0184446873b905e75c",
            "1b62ec5fcf004c16a67a27f573c3a473",
            "9348ec75ca5947d4b8a562de648ba70f",
            "681ba9f77eda4b39b0230b1de972584d",
            "11551c0dfe8843f0970951466f94bf3b",
            "3e4deca72a9d4f40b2502321d57ba205",
            "03228168c8944ac886e790652f8ef346",
            "23a28fa1c34948e0a25a7962ddc89f6c",
            "578cf5176573420da54c6153de442127",
            "afe8705e1ea84831bee366618675ea3c",
            "49a3577759144c08a8b44d9b7aacc7d9",
            "3de23673e0954574a59232ed6cff272d",
            "8d82135eb33f4366a9671bccacc29725",
            "62dc9819462a43278a3687c4c7c982a2",
            "5f5bdf9a7470438cb155cd884877878e",
            "be10caf3d45f41238f40e991bb9a8768",
            "142025b648cd460ba8e72a031f779e4f",
            "9008f44d91de4b9cb6c5d963abe222c5",
            "207d718eec584ea4be75995afb4bdc3e",
            "6360d4152e1e4c27aced3b23991a943e",
            "89f279e0290443679516d7caf6347f03",
            "c318cd8e4cd444ba937c8266bdc2c69e",
            "51887049a91b4d3ca2351f3de42e5d61",
            "7c4664f39e8541de98c2a74489d0c30e",
            "0dfd9ce308fb4b50af461161c3fe26e7",
            "65f49c4fb50743f2b971bb1dc35dcda9",
            "c716a95f3dc74fd9bf58b248c17584f6",
            "2f850537f4734409b0f2c7965438936f",
            "665dae87fa3148539b2c061001db63b4",
            "13501b8b915a45aba320d5c0c07b7091",
            "2093facb91c84defac44a8bdadf17696",
            "57ba0f9e5c6c4a618753e5bb83751c8a",
            "6c285b0bcba042628addb82552616055",
            "85c7f405e0014574a17e3503be286043",
            "c4f8f7070f254483acd4cb25faba7815",
            "9516dc0ea49241658fd825f0c3a9ee10",
            "a899d0724daf42ecab15b508a9a22acc",
            "8e6916d567be40c4b2e66bc2c178b781",
            "4c6d77ad345c4e87abf336ec3696d8af",
            "4c254fdb9dd4464ca685ea352e529f1c",
            "f720b6f2c0fc49bbbdadb107eefb1082",
            "6822afd19f9f439fb82768675f7ccca6",
            "f5fa1a1661ad453fb73ff2099d88b587",
            "d1468bdad8f9445dab5c67f740a12203",
            "7d25077131c5462ebda4fecd0dfedec5",
            "1dbca783356440a1943d09d9671b8746",
            "71962cfcc4714e95b4c01bca13fe2895",
            "8560908816e043919c26ff6fb749215c",
            "1dc7ff7de11d45b885b0d78bd2e0b1e5",
            "45ba2f34fca0446c8115231e9225e434",
            "5a1dd345d8494ddf958499cbb702935d",
            "ad9659bf053142c2997ec7a0f13dfd16",
            "efd70cb4a4eb421b82fa8538ff8f6c6b",
            "3f6c02af73f74e15be319dc5d067fd00",
            "682052d8dda44549be8bba69c140afef",
            "fd3b24e1168d4e8b95f6fea20251c224",
            "37fe9f269a94409b9adc1cd912261d95",
            "7f4791b9a2bd447caa1bee491d537474",
            "30b420cfd7e946cea05cce5cedeab942",
            "9da107bd5da147daa04ccfc96f787990",
            "9fec9a54a4af49539700935a5c83e907",
            "53283e3fa45b460bad8001e28c9b158b"
          ]
        },
        "id": "f9fde54d",
        "outputId": "241367b6-b8a3-4611-9da8-ca01606272ae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6053429fcf774bb1a81334d51907fc3d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9a0f5e9fbf145fbbcbac9403b2ac49d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60d95e191a134dc7a48a2f9c81084537"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e571033cdb843fc88f3027de180f3b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03dea5a1c59f40a88046a2ad630cc00d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27eff59366f74102b6d339fd1b49c56f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77a6f84499e145019729ff4326d94b15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23a28fa1c34948e0a25a7962ddc89f6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "207d718eec584ea4be75995afb4bdc3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13501b8b915a45aba320d5c0c07b7091"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f720b6f2c0fc49bbbdadb107eefb1082"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad9659bf053142c2997ec7a0f13dfd16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building indexes (this may take a moment)...\n",
            "\u2705 Indexes built.\n"
          ]
        }
      ],
      "source": [
        "# 1. TF-IDF Setup\n",
        "def build_tfidf_index(texts: List[str]):\n",
        "    vec = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
        "    X = vec.fit_transform(texts)\n",
        "    X = normalize(X)\n",
        "    return vec, X\n",
        "\n",
        "# 2. BM25 Setup\n",
        "def build_bm25_index(texts: List[str]):\n",
        "    tokenized_corpus = [doc.split(\" \") for doc in texts]\n",
        "    return BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# 3. Dense Setup\n",
        "model_st = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "def build_dense_index(texts: List[str]):\n",
        "    embeddings = model_st.encode(texts, convert_to_numpy=True)\n",
        "    normalize(embeddings, copy=False)\n",
        "    d = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(d)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "print(\"Building indexes (this may take a moment)...\")\n",
        "\n",
        "# PAGE indexes\n",
        "page_texts = [c.text for c in page_chunks]\n",
        "tfidf_vec_page, tfidf_X_page = build_tfidf_index(page_texts)\n",
        "bm25_page = build_bm25_index(page_texts)\n",
        "dense_index_page = build_dense_index(page_texts)\n",
        "\n",
        "# FIXED indexes\n",
        "fixed_texts = [c.text for c in fixed_chunks]\n",
        "tfidf_vec_fixed, tfidf_X_fixed = build_tfidf_index(fixed_texts)\n",
        "bm25_fixed = build_bm25_index(fixed_texts)\n",
        "dense_index_fixed = build_dense_index(fixed_texts)\n",
        "\n",
        "# IMAGE indexes\n",
        "img_texts = [it.caption for it in image_items]\n",
        "tfidf_vec_img, tfidf_X_img = build_tfidf_index(img_texts)\n",
        "dense_index_img = build_dense_index(img_texts)\n",
        "\n",
        "print(\"\u2705 Indexes built.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58L_F3zd8pkz"
      },
      "source": [
        "### Cell Description: Indexing (Sparse + Dense)\n",
        "\n",
        "- **What this cell does:** Builds Sparse (TF-IDF, BM25) and Dense (SentenceTransformers) indices for both text chunks and image captions.\n",
        "- **Why it matters:** Enables comparison between keyword-based retrieval (good for exact terms) and semantic retrieval (good for concepts). Multimodal indexing allows retrieving images via text queries.\n",
        "- **Key assumptions:** `all-MiniLM-L6-v2` provides sufficient semantic understanding for this domain."
      ],
      "id": "58L_F3zd8pkz"
    },
    {
      "cell_type": "markdown",
      "id": "d14a7a9b",
      "metadata": {
        "id": "d14a7a9b"
      },
      "source": [
        "## 6) Build evidence context\n",
        "We assemble a compact context string + list of image paths.\n",
        "\n",
        "**Guidelines for good context:**\n",
        "- Keep snippets short (100\u2013300 chars)\n",
        "- Always include chunk IDs so you can cite evidence\n",
        "- Attach images that are likely relevant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "14f595da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548,
          "referenced_widgets": [
            "47b5bac3101a4a449699c5936523b580",
            "807cd8b1482c43b68576c0567e486519",
            "9aa47a92309749ac885502f677b9c48b",
            "6bae8a5763834b63bb94fe12a1b57d35",
            "1697f50a26464f8d89b3185c862a86ce",
            "6c65c43319bf48b98c01efa068ba726c",
            "17b8f9938b51475c96a249322e4c996a",
            "82bfcf290ab548909bc19d546154e00c",
            "b8eb609a1f29445fb8e126fef47f19f7",
            "02ceec27f1e64f8fa7898fe23ee84b53",
            "9e449e0f02514c34a4bdec28a7eebdf8",
            "3e8c2761a0704af096a2e21484548207",
            "3cf43ef0a5834dc1b143111852b9b32a",
            "a6668397dde6412bbf9bcd6777e24b47",
            "4e19567ca2d0425591b3b2b132564c37",
            "38e0dd29d96b4ce29445ba194e2d2d84",
            "7663757eb709415aa7c9e41daec3c260",
            "f84e2353e9e74d13af3b4568b63cca2a",
            "2501661735c049fcb08132bb68041493",
            "73652595b56448aab4f1281fdd8a74bd",
            "13d4c0fa0a3441f1ad562ccef5731124",
            "374500c7f3ce4188ba406180c44c2022",
            "70c47be8b1144f369ea7f46a372037cf",
            "77e0b41ad1b243c7ab742a4b9bfa18b4",
            "fcef134f97c1470e95fd010e0b9e7844",
            "c62030c563a94a07bb5ae41765198fbd",
            "4f266c1232384de9a3c6843ce4854acc",
            "9e2ac7c11dd9425391d8ed638a45fd93",
            "1bf06d0574774b2d89c86c64ce7475d4",
            "b97044b0f34b4392804237656e110f7f",
            "f50017e0f76347bc81b3afcdb81153f7",
            "9a8c1a0495534153a6c6c061d3f7bd34",
            "c82816f22cdb48e5bd7b7ecdabba1d0c",
            "fb68554241a346e590ef82fd6f2a5853",
            "3cb51c16e02d455fabb985172e4432c0",
            "2a13f76e3ab54dfaa08b68a11a7fe865",
            "5d67f170a72a4c1aab4c4232f1703c03",
            "23eafa7b9c65458cb9320f717b21ba9e",
            "60f1079bd7df4641a34d89923fb2c2ac",
            "a74e263747074d1c9e8ea6fc54cbd883",
            "64678a2616934b20a93cf085a68eec86",
            "e617c2271980483d95dacc0d35ee969b",
            "9e8489189ca845fd92c88af3346453c8",
            "9bc8123be4da4903972f1680bb5c6b17",
            "ed6a5f2bec744e2885daced235f05f62",
            "9cf7438cde754961885b4be6381ee717",
            "5a5044b1f131408ba70aa728a63bdb1f",
            "a3624df41c8a4efa8afd4a4f823dc98a",
            "2fd75d09b36946cda92d8f2cc97659f2",
            "b3a8a16198a84968b934977d0026d03c",
            "1d0cafc4a996443c8e1232d325b6e47b",
            "12cc82520b17452896c67738ebf7a32b",
            "2b07f34d56fd4c9b8457fa608b481a04",
            "42f7a562058a4f73a612f15c0aa3df42",
            "58957de2119e45a5b99d9d195b0322ae",
            "20d81319204f4e759a12e5f4e4eb6e43",
            "8b16bfb63dfe48c7876b626599152d74",
            "c876ad9e5be24b48b38ed681a9093757",
            "5909e250dce84795b9a885cb27f08358",
            "27b7be7a25f94e3b9f39a1504021ffbb",
            "38f55380eed245ebbb13e132dbcdb485",
            "a248443bdb9b40e4b63915526fd62f4e",
            "3eee3a188bf448c28bd81adb24038ae8",
            "44e02617bd3d46efba78ff070fe103d0",
            "73909491363e450aa64220668a866bcc",
            "8fad4a7e6e1247489d73bfb16c723592",
            "f94b30beeeab4195ad2e11ca36e0250f",
            "c2497c775c494b82aa0cdec74fcb448a",
            "f82d75064314419589625bc1b0df5682",
            "be15e4141a2a4c32b4ae6bdef41d3b91",
            "2dba605a8d8c4c8ebfccdcc112714103",
            "cb217dc5a1f644dda04ccca23c9d2da9",
            "043fa5681ca04d018c9e6b099c7c0f3c",
            "3326260a289048ad9b723a04806f1e48",
            "38bbdaf1b48d472081e51826c246a45e",
            "82fffb329d8c4c3d98d7ff2e553451ef",
            "6863848c42bc4158940b54732335d459",
            "d0f8a009409643bfbb220580f204b702",
            "58fb5a1ef4e4405688ace60bc3abf631",
            "198fd4c0d5db45efae7af295cfaa2c21",
            "b4e35d7cc0fb4770848729d3bef7b2e1",
            "b4163f95bff8407bb981f11f7a6e27d5",
            "c5f3768c935e48259616530a819b442c",
            "1f788af89d784c7fa2c62ee10a294d32",
            "5ef260e4a1a34440af2b706e6e33b8d7",
            "9791f27197f743e6a4dfb533a121fc46",
            "7ebaa102988b47e1a27e8b3966d8a9ac",
            "e6491caf375f486dab16d371822395ef"
          ]
        },
        "id": "14f595da",
        "outputId": "ebb3611b-290b-4e85-92ad-d44d11f47d09"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47b5bac3101a4a449699c5936523b580"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e8c2761a0704af096a2e21484548207"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/105 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70c47be8b1144f369ea7f46a372037cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertForSequenceClassification LOAD REPORT from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "Key                          | Status     |  | \n",
            "-----------------------------+------------+--+-\n",
            "bert.embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb68554241a346e590ef82fd6f2a5853"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed6a5f2bec744e2885daced235f05f62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20d81319204f4e759a12e5f4e4eb6e43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f94b30beeeab4195ad2e11ca36e0250f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0f8a009409643bfbb220580f204b702"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[IMAGE | img1.png | score=-7.435] caption=img1\n",
            "[IMAGE | img10.png | score=-7.684] caption=img10\n",
            "[IMAGE | img2.png | score=-7.726] caption=img2\n",
            "[TEXT | doc3.pdf::p29 | score=-7.900] NIST CSWP 29 The NIST Cybersecurity Framework (CSF) 2.0 February 26, 2024 24 Appendix B. CSF Tiers Table 2 contains a notional illustration of the CSF Tiers discussed in Sec. 3. The Tiers characterize the rigor of an organization\u2019s cybersecurity risk governanc\n",
            "[TEXT | doc3.pdf::p17 | score=-8.217] NIST CSWP 29 The NIST Cybersecurity Framework (CSF) 2.0 February 26, 2024 12 organizations by their nature may monitor risk at the enterprise level, while larger companies may maintain separate risk management efforts integrated into the ERM. Organizations can\n",
            "[TEXT | doc3.pdf::p16 | score=-8.248] NIST CSWP 29 The NIST Cybersecurity Framework (CSF) 2.0 February 26, 2024 11 Preparing to create and use Organizational Profiles involves gathering information about organizational priorities, resources, and risk direction from executives. Managers then collab\n",
            "[TEXT | doc3.pdf::p30 | score=-9.123] NIST CSWP 29 The NIST Cybersecurity Framework (CSF) 2.0 February 26, 2024 25 Tier Cybersecurity Risk Governance Cybersecurity Risk Management The organization risk strategy is informed by the cybersecurity risks associated with its suppliers and the products a\n",
            "[TEXT | doc3.pdf::p13 | score=-9.834] NIST CSWP 29 The NIST Cybersecurity Framework (CSF) 2.0 February 26, 2024 8 continuously improving. Selecting Tiers helps set the overall tone for how an organization will manage its cybersecurity risks. Fig. 4. CSF Tiers for cybersecurity risk governance and \n"
          ]
        }
      ],
      "source": [
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "def retrieve_tfidf(query: str, vec, X, top_k=5):\n",
        "    q = vec.transform([query])\n",
        "    q = normalize(q)\n",
        "    scores = (X @ q.T).toarray().ravel()\n",
        "    idx = np.argsort(-scores)[:top_k]\n",
        "    return [(int(i), float(scores[i])) for i in idx]\n",
        "\n",
        "def retrieve_bm25(query: str, bm25_obj, top_k=5):\n",
        "    tokenized_query = query.split(\" \")\n",
        "    scores = bm25_obj.get_scores(tokenized_query)\n",
        "    idx = np.argsort(-scores)[:top_k]\n",
        "    return [(int(i), float(scores[i])) for i in idx]\n",
        "\n",
        "def retrieve_dense(query: str, index, top_k=5):\n",
        "    q_emb = model_st.encode([query], convert_to_numpy=True)\n",
        "    normalize(q_emb, copy=False)\n",
        "    scores, indices = index.search(q_emb, top_k)\n",
        "    return [(int(indices[0][i]), float(scores[0][i])) for i in range(top_k) if indices[0][i] != -1]\n",
        "\n",
        "def build_context(\n",
        "    question: str,\n",
        "    method: str = \"sparse\",\n",
        "    chunking: str = \"page\",\n",
        "    top_k_text: int = TOP_K_TEXT,\n",
        "    top_k_images: int = TOP_K_IMAGES,\n",
        "    top_k_evidence: int = TOP_K_EVIDENCE,\n",
        "    alpha: float = ALPHA,\n",
        ") -> Dict[str, Any]:\n",
        "    # Select Corpus\n",
        "    if chunking == \"page\":\n",
        "        chunks = page_chunks\n",
        "        tfidf_vec, tfidf_X = tfidf_vec_page, tfidf_X_page\n",
        "        bm25_obj = bm25_page\n",
        "        dense_idx = dense_index_page\n",
        "    else:\n",
        "        chunks = fixed_chunks\n",
        "        tfidf_vec, tfidf_X = tfidf_vec_fixed, tfidf_X_fixed\n",
        "        bm25_obj = bm25_fixed\n",
        "        dense_idx = dense_index_fixed\n",
        "\n",
        "    # 1. Text Retrieval\n",
        "    text_hits = []\n",
        "    if method == \"sparse\":\n",
        "        text_hits = retrieve_tfidf(question, tfidf_vec, tfidf_X, top_k=top_k_text)\n",
        "    elif method == \"bm25\":\n",
        "        text_hits = retrieve_bm25(question, bm25_obj, top_k=top_k_text)\n",
        "    elif method == \"dense\":\n",
        "        text_hits = retrieve_dense(question, dense_idx, top_k=top_k_text)\n",
        "    elif \"hybrid\" in method:\n",
        "        h1 = retrieve_tfidf(question, tfidf_vec, tfidf_X, top_k=top_k_text * 2)\n",
        "        h2 = retrieve_dense(question, dense_idx, top_k=top_k_text * 2)\n",
        "        # Simple rank fusion or score fusion (using dict)\n",
        "        combined = {}\n",
        "        for i, s in h1: combined[i] = combined.get(i, 0) + 0.3 * s\n",
        "        for i, s in h2: combined[i] = combined.get(i, 0) + 0.7 * s\n",
        "        text_hits = sorted(combined.items(), key=lambda x: x[1], reverse=True)[:top_k_text]\n",
        "\n",
        "    # Prepare candidates for fusion/reranking\n",
        "    candidates = []\n",
        "    for idx, s in text_hits:\n",
        "        candidates.append({\n",
        "            \"modality\": \"text\",\n",
        "            \"id\": chunks[idx].chunk_id,\n",
        "            \"score\": float(s),\n",
        "            \"text\": chunks[idx].text,\n",
        "            \"path\": None\n",
        "        })\n",
        "\n",
        "    # 2. Image Retrieval\n",
        "    img_hits = retrieve_tfidf(question, tfidf_vec_img, tfidf_X_img, top_k=top_k_images)\n",
        "    for idx, s in img_hits:\n",
        "        candidates.append({\n",
        "            \"modality\": \"image\",\n",
        "            \"id\": image_items[idx].item_id,\n",
        "            \"score\": float(s),\n",
        "            \"text\": image_items[idx].caption,\n",
        "            \"path\": image_items[idx].path\n",
        "        })\n",
        "\n",
        "    # 3. Rerank (if requested)\n",
        "    if \"rerank\" in method:\n",
        "        pairs = [[question, c[\"text\"]] for c in candidates]\n",
        "        scores = cross_encoder.predict(pairs)\n",
        "        for i, s in enumerate(scores): candidates[i][\"score\"] = float(s)\n",
        "        candidates.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "    else:\n",
        "        # Basic normalize & sort\n",
        "        candidates.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "    # 4. Final Selection\n",
        "    final_evidence = candidates[:top_k_evidence]\n",
        "    ctx_lines = []\n",
        "    image_paths = []\n",
        "    for ev in final_evidence:\n",
        "        if ev[\"modality\"] == \"text\":\n",
        "            snippet = (ev[\"text\"] or \"\")[:260].replace(\"\\n\", \" \")\n",
        "            ctx_lines.append(f\"[TEXT | {ev['id']} | score={ev['score']:.3f}] {snippet}\")\n",
        "        else:\n",
        "            ctx_lines.append(f\"[IMAGE | {ev['id']} | score={ev['score']:.3f}] caption={ev['text']}\")\n",
        "            image_paths.append(ev[\"path\"])\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"context\": \"\\n\".join(ctx_lines),\n",
        "        \"image_paths\": image_paths,\n",
        "        \"evidence\": final_evidence,\n",
        "        \"method\": method\n",
        "    }\n",
        "\n",
        "# Demo\n",
        "ctx_demo = build_context(QUERIES[0][\"question\"], method=\"hybrid_rerank\")\n",
        "print(ctx_demo[\"context\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAqewA-M8pk1"
      },
      "source": [
        "### Cell Description: Retrieval Logic (Context Building)\n",
        "\n",
        "- **What this cell does:** The core retrieval logic that fetches candidates, fuses scores (Hybrid), optionally reranks, and constructs the prompt context.\n",
        "- **Why it matters:** This is the 'Brain' of the RAG system. It determines what evidence is presented to the LLM. Hybrid fusion allows leveraging strengths of both sparse and dense retrievers.\n",
        "- **Key assumptions:** Normalization of scores (0-1) is sufficient for fair fusion between different retrieval methods."
      ],
      "id": "GAqewA-M8pk1"
    },
    {
      "cell_type": "markdown",
      "id": "373612a5",
      "metadata": {
        "id": "373612a5"
      },
      "source": [
        "## 7) \u201cGenerator\u201d (simple, offline)\n",
        "To keep this notebook runnable anywhere, we implement a **lightweight extractive generator**:\n",
        "- It returns the top evidence lines\n",
        "- In your real submission, you can replace this with an LLM call (HF local model or an API)\n",
        "\n",
        "**Key rule:** the answer must stay consistent with evidence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a34c57e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a34c57e9",
        "outputId": "8505fedc-674d-4142-9bb7-a3d8f488587c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Q1 Based on the risk matrix shown in the figures and the accompanying text, which combination of likelihood and impact corresponds to the highest risk level?\n",
            "Question: Based on the risk matrix shown in the figures and the accompanying text, which combination of likelihood and impact corresponds to the highest risk level?\n",
            "\n",
            "Grounded answer (extractive):\n",
            "[TEXT | doc1.pdf::p11::c4500 | score=-6.947]  for which a health care facility or business associate, as applicable, determines there is a low probability of compromise in accordance with HIPAA\u2019s 4-factor risk assessment (see \u201cAnalysis of Risk of Harm\u201d section below for a complete listing of these facto\n",
            "Images: ['img1.png', 'img10.png', 'img2.png']\n",
            "\n",
            "================================================================================\n",
            "Q2 Using both the Zero Trust architecture diagram and the document text, what core principle is emphasized for access decisions?\n",
            "Question: Using both the Zero Trust architecture diagram and the document text, what core principle is emphasized for access decisions?\n",
            "\n",
            "Grounded answer (extractive):\n",
            "[TEXT | doc4.pdf::p45::c0 | score=0.097] 45 where they can be securely communicated to a platform. The foundational building block that zero trust architecture usually consists of several aspects: \u2022 Each entity can create proof of whom the identity is \u2022 Entities can independently authenticate other i\n",
            "[TEXT | doc4.pdf::p44::c1500 | \n",
            "Images: ['img1.png', 'img2.png', 'img10.png']\n",
            "\n",
            "================================================================================\n",
            "Q3 What specific encryption algorithm (for example, AES-256 or RSA-2048) is mandated by the organization\u2019s policy?\n",
            "Question: What specific encryption algorithm (for example, AES-256 or RSA-2048) is mandated by the organization\u2019s policy?\n",
            "\n",
            "Grounded answer (extractive):\n",
            "[TEXT | doc5.pdf::p33::c3000 | score=-3.432] Inspected the company's Cryptography Policy to determine that the company restricts privileged access to encryption keys to authorized users with a business need. No exceptions noted.\n",
            "[TEXT | doc4.pdf::p28::c0 | score=-3.856] 28 path, size, and frequency of access as well regulations, compliance or ad\n",
            "Images: ['img1.png', 'img2.png', 'img10.png']\n"
          ]
        }
      ],
      "source": [
        "def simple_extractive_answer(question: str, context: str) -> str:\n",
        "    lines = context.splitlines()\n",
        "    if not lines:\n",
        "        return \"I don't know (no evidence retrieved).\"\n",
        "    # Return top 2 evidence lines as a \"grounded\" answer\n",
        "    return (\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        \"Grounded answer (extractive):\\n\"\n",
        "        + \"\\n\".join(lines[:2])\n",
        "    )\n",
        "\n",
        "# --- OPTIONAL: Gemini API Hook (Uncomment to use) ---\n",
        "# import google.generativeai as genai\n",
        "# GOOGLE_API_KEY = \"YOUR_KEY_HERE\"\n",
        "# genai.configure(api_key=GOOGLE_API_KEY)\n",
        "# model = genai.GenerativeModel('gemini-pro')\n",
        "#\n",
        "# def generate_gemini(question, context):\n",
        "#     prompt = f\"Based on the following evidence, answer the question. Cite sources like [doc1] or [img1].\\nEvidence:\\n{context}\\nQuestion: {question}\"\n",
        "#     return model.generate_content(prompt).text\n",
        "\n",
        "def run_query(qobj, top_k_text=TOP_K_TEXT, top_k_images=TOP_K_IMAGES, top_k_evidence=TOP_K_EVIDENCE, alpha=ALPHA) -> Dict[str, Any]:\n",
        "    question = qobj[\"question\"]\n",
        "    # Use the advanced build_context if available, else fallback\n",
        "    try:\n",
        "        ctx = build_context(question, method=\"hybrid_rerank\", chunking=\"fixed\")\n",
        "    except NameError:\n",
        "        # Fallback to simple if advanced not defined yet (during partial runs)\n",
        "        ctx = build_context(question, top_k_text=top_k_text, top_k_images=top_k_images, top_k_evidence=top_k_evidence, alpha=alpha)\n",
        "\n",
        "    # Use Gemini if available, else simple\n",
        "    # answer = generate_gemini(question, ctx[\"context\"])\n",
        "    answer = simple_extractive_answer(question, ctx[\"context\"])\n",
        "\n",
        "    return {\n",
        "        \"id\": qobj[\"id\"],\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"context\": ctx[\"context\"],\n",
        "        \"image_paths\": ctx[\"image_paths\"],\n",
        "    }\n",
        "\n",
        "results = [run_query(q) for q in QUERIES]\n",
        "for r in results:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(r[\"id\"], r[\"question\"])\n",
        "    print(r[\"answer\"][:500])\n",
        "    print(\"Images:\", [os.path.basename(p) for p in r[\"image_paths\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSUuAvUe8pk2"
      },
      "source": [
        "### Cell Description: Generator\n",
        "\n",
        "- **What this cell does:** Generates an answer based on the retrieved context. Currently implements a simple extractive baseline, with hooks for an LLM.\n",
        "- **Why it matters:** Demonstrates the End-to-End RAG flow. In a production system, this would be replaced by a generative model to synthesize the answer.\n",
        "- **Tradeoffs:** The extractive baseline cannot synthesize new information, only repeat evidence."
      ],
      "id": "oSUuAvUe8pk2"
    },
    {
      "cell_type": "markdown",
      "id": "9a4ba05a",
      "metadata": {
        "id": "9a4ba05a"
      },
      "source": [
        "## 8) Retrieval Evaluation (Precision@k / Recall@k)\n",
        "We treat a text chunk as **relevant** for a query if it contains at least one `must_have_keywords` term.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2d16c336",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "2d16c336",
        "outputId": "2cc459e1-c8e2-48c0-f11b-57434fa9fbd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id  P@5      R@10  total_relevant_chunks\n",
              "0  Q1  0.4  0.119048                     42\n",
              "1  Q2  0.6  0.200000                     25\n",
              "2  Q3  0.2  1.000000                      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4adfc664-839f-44b1-995c-d54598a11337\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>P@5</th>\n",
              "      <th>R@10</th>\n",
              "      <th>total_relevant_chunks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Q1</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.119048</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Q2</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Q3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4adfc664-839f-44b1-995c-d54598a11337')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4adfc664-839f-44b1-995c-d54598a11337 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4adfc664-839f-44b1-995c-d54598a11337');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_431b18fe-484f-424a-9529-792c2363caef\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_eval')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_431b18fe-484f-424a-9529-792c2363caef button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_eval');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_eval",
              "summary": "{\n  \"name\": \"df_eval\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Q1\",\n          \"Q2\",\n          \"Q3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"P@5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19999999999999998,\n        \"min\": 0.2,\n        \"max\": 0.6,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.4,\n          0.6,\n          0.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4869343531195159,\n        \"min\": 0.11904761904761904,\n        \"max\": 1.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.11904761904761904,\n          0.2,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_relevant_chunks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20,\n        \"min\": 1,\n        \"max\": 42,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          42,\n          25,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "def is_relevant_text(chunk_text: str, rubric: Dict[str, Any]) -> bool:\n",
        "    text = chunk_text.lower()\n",
        "    must = [k.lower() for k in rubric.get(\"must_have_keywords\", [])]\n",
        "    return any(k in text for k in must)\n",
        "\n",
        "def precision_at_k(relevances: List[bool], k: int) -> float:\n",
        "    k = min(k, len(relevances))\n",
        "    if k == 0:\n",
        "        return 0.0\n",
        "    return sum(relevances[:k]) / k\n",
        "\n",
        "def recall_at_k(relevances: List[bool], k: int, total_relevant: int) -> float:\n",
        "    k = min(k, len(relevances))\n",
        "    if total_relevant == 0:\n",
        "        return 0.0\n",
        "    return sum(relevances[:k]) / total_relevant\n",
        "\n",
        "def eval_retrieval_for_query(qobj, top_k=10) -> Dict[str, Any]:\n",
        "    question = qobj[\"question\"]\n",
        "    rubric = qobj[\"rubric\"]\n",
        "\n",
        "    # Corrected: Using retrieve_tfidf and the globally defined page TF-IDF vectors\n",
        "    hits = retrieve_tfidf(question, tfidf_vec_page, tfidf_X_page, top_k=top_k)\n",
        "    rels = []\n",
        "    for i, score in hits:\n",
        "        rels.append(is_relevant_text(page_chunks[i].text, rubric))\n",
        "\n",
        "    # Estimate total relevant in the corpus (for recall)\n",
        "    total_rel = sum(is_relevant_text(ch.text, rubric) for ch in page_chunks)\n",
        "\n",
        "    return {\n",
        "        \"id\": qobj[\"id\"],\n",
        "        \"P@5\": precision_at_k(rels, 5),\n",
        "        \"R@10\": recall_at_k(rels, 10, total_rel),\n",
        "        \"total_relevant_chunks\": total_rel,\n",
        "    }\n",
        "\n",
        "eval_rows = [eval_retrieval_for_query(q) for q in QUERIES]\n",
        "df_eval = pd.DataFrame(eval_rows)\n",
        "df_eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnPdE2S28pk3"
      },
      "source": [
        "### Cell Description: Retrieval Metrics\n",
        "\n",
        "- **What this cell does:** Calculates retrieval metrics (Precision@5, Recall@10) based on keyword matching against the defined rubric.\n",
        "- **Why it matters:** Quantifies the performance of the retrieval system, allowing for data-driven improvements.\n",
        "- **Key assumptions:** Keyword matching is a proxy for true relevance (which would require human annotation)."
      ],
      "id": "CnPdE2S28pk3"
    },
    {
      "cell_type": "markdown",
      "id": "de705dbf",
      "metadata": {
        "id": "de705dbf"
      },
      "source": [
        "## 9) Ablation Study (REQUIRED)\n",
        "\n",
        "You must compare **at least**:\n",
        "- **Chunking A (page-based)** vs **Chunking B (fixed-size)**  \n",
        "- **Sparse** vs **Dense** vs **Hybrid** vs **Hybrid + Rerank** *(dense/rerank can be optional extensions \u2014 but include at least sparse + one fusion variant)*  \n",
        "- **Text-only RAG** vs **Multimodal RAG** (your context must include evidence items)\n",
        "\n",
        "**Deliverable:** include a final results table in your README:\n",
        "\n",
        "`Query \u00d7 Method \u00d7 Precision@5 \u00d7 Recall@10 \u00d7 Faithfulness`\n",
        "\n",
        "### Quick ablation ideas\n",
        "- Vary `TOP_K_TEXT`: 2, 5, 10  \n",
        "- Vary `ALPHA`: 0.2, 0.5, 0.8  \n",
        "- Compare page-chunking vs fixed-size (`CHUNK_SIZE` / `CHUNK_OVERLAP`)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d8b191c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8b191c1",
        "outputId": "8926b64d-c4c8-427a-d1d8-4b10332742bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Query         Method Chunking  P@5      R@10  Faithfulness\n",
            "0     Q1         sparse     page  0.4  0.666667      0.666667\n",
            "1     Q1           bm25     page  1.0  0.666667      0.666667\n",
            "2     Q1          dense     page  0.4  0.333333      0.666667\n",
            "3     Q1         hybrid     page  0.2  0.333333      0.666667\n",
            "4     Q1  hybrid_rerank     page  0.2  0.333333      0.666667\n",
            "5     Q1         sparse    fixed  0.4  0.666667      0.666667\n",
            "6     Q1           bm25    fixed  0.8  0.666667      0.666667\n",
            "7     Q1          dense    fixed  0.2  0.333333      0.666667\n",
            "8     Q1         hybrid    fixed  0.2  0.333333      0.666667\n",
            "9     Q1  hybrid_rerank    fixed  0.2  0.333333      0.666667\n",
            "10    Q2         sparse     page  0.6  0.333333      0.333333\n",
            "11    Q2           bm25     page  0.6  0.333333      0.333333\n",
            "12    Q2          dense     page  0.2  0.333333      0.333333\n",
            "13    Q2         hybrid     page  0.2  0.333333      0.333333\n",
            "14    Q2  hybrid_rerank     page  0.2  0.333333      0.333333\n",
            "15    Q2         sparse    fixed  0.8  0.333333      0.333333\n",
            "16    Q2           bm25    fixed  0.6  0.333333      0.333333\n",
            "17    Q2          dense    fixed  0.4  0.333333      0.333333\n",
            "18    Q2         hybrid    fixed  0.6  0.333333      0.333333\n",
            "19    Q2  hybrid_rerank    fixed  0.6  0.333333      0.333333\n",
            "20    Q3         sparse     page  0.2  0.333333      1.000000\n",
            "21    Q3           bm25     page  0.2  0.333333      1.000000\n",
            "22    Q3          dense     page  0.2  0.333333      1.000000\n",
            "23    Q3         hybrid     page  0.2  0.333333      1.000000\n",
            "24    Q3  hybrid_rerank     page  0.2  0.333333      1.000000\n",
            "25    Q3         sparse    fixed  0.2  0.333333      1.000000\n",
            "26    Q3           bm25    fixed  0.0  0.000000      1.000000\n",
            "27    Q3          dense    fixed  0.2  0.333333      1.000000\n",
            "28    Q3         hybrid    fixed  0.2  0.333333      1.000000\n",
            "29    Q3  hybrid_rerank    fixed  0.2  0.333333      1.000000\n"
          ]
        }
      ],
      "source": [
        "def calculate_faithfulness(answer: str, rubric: Dict[str, Any]) -> float:\n",
        "    \"\"\"Calculates a raw faithfulness score based on keyword overlap.\"\"\"\n",
        "    if not answer:\n",
        "        return 0.0\n",
        "\n",
        "    answer_lower = answer.lower()\n",
        "    must_have = [k.lower() for k in rubric.get(\"must_have_keywords\", [])]\n",
        "    if not must_have:\n",
        "        return 1.0 # No requirements\n",
        "\n",
        "    matches = sum(1 for k in must_have if k in answer_lower)\n",
        "    return matches / len(must_have)\n",
        "\n",
        "def run_ablation():\n",
        "    results = []\n",
        "    methods = [\"sparse\", \"bm25\", \"dense\", \"hybrid\", \"hybrid_rerank\"]\n",
        "    strategies = [\"page\", \"fixed\"]\n",
        "\n",
        "    for q in QUERIES:\n",
        "        for strat in strategies:\n",
        "            for meth in methods:\n",
        "                # Retrieve (Ensure enough evidence is fetched for P@10)\n",
        "                ctx = build_context(q[\"question\"], method=meth, chunking=strat, top_k_evidence=10)\n",
        "\n",
        "                # Generate Answer (Extractive)\n",
        "                answer = simple_extractive_answer(q[\"question\"], ctx[\"context\"])\n",
        "\n",
        "                # Evaluate Retrieval (P@5 and R@10)\n",
        "                rubric = q[\"rubric\"]\n",
        "                must_have = [k.lower() for k in rubric.get(\"must_have_keywords\", [])]\n",
        "                retrieved_rels = []\n",
        "                found_keywords = set()\n",
        "\n",
        "                for i, ev in enumerate(ctx[\"evidence\"]):\n",
        "                    if ev[\"modality\"] == \"text\":\n",
        "                        text_lower = ev[\"text\"].lower()\n",
        "                        is_rel = any(k in text_lower for k in must_have)\n",
        "                        retrieved_rels.append(is_rel)\n",
        "                        if i < 10:\n",
        "                             found_keywords.update(k for k in must_have if k in text_lower)\n",
        "                    else:\n",
        "                        retrieved_rels.append(False)\n",
        "\n",
        "                p5 = sum(retrieved_rels[:5]) / 5 if retrieved_rels else 0\n",
        "                r10 = len(found_keywords) / len(must_have) if must_have else 0\n",
        "\n",
        "                # Evaluate Faithfulness\n",
        "                faith_score = calculate_faithfulness(answer, rubric)\n",
        "\n",
        "                results.append({\n",
        "                    \"Query\": q[\"id\"],\n",
        "                    \"Method\": meth,\n",
        "                    \"Chunking\": strat,\n",
        "                    \"P@5\": p5,\n",
        "                    \"R@10\": r10,\n",
        "                    \"Faithfulness\": faith_score\n",
        "                })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "df_res = run_ablation()\n",
        "print(df_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UInhyoY58pk4"
      },
      "source": [
        "### Cell Description: Ablation Study\n",
        "\n",
        "- **What this cell does:** Systematically compares different configurations (Chunking strategies \u00d7 Retrieval methods) to identify the optimal setup.\n",
        "- **Why it matters:** Provides scientific rigor to system design choices, proving which components contribute to performance.\n",
        "- **Tradeoffs:** Only evaluates a subset of possible hyperparameters due to compute/time constraints."
      ],
      "id": "UInhyoY58pk4"
    },
    {
      "cell_type": "markdown",
      "id": "652c1e2d",
      "metadata": {
        "id": "652c1e2d"
      },
      "source": [
        "## 10) What to submit\n",
        "1) Your updated dataset (or keep your own)\n",
        "2) This notebook (with your answers + screenshots/outputs)\n",
        "3) A short write\u2011up: retrieval metrics + faithfulness discussion + ablation\n",
        "\n",
        "**Tip:** If you switch to an LLM, keep the same `build_context()` so the evidence is always visible.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}